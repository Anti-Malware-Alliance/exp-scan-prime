{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_int(hex_value):\n",
    "    try:\n",
    "        # Remove potential '0x' prefix and convert to int\n",
    "        return int(hex_value, 16)\n",
    "    except (ValueError, TypeError):  # Handle non-convertible or empty values\n",
    "        return None\n",
    "    \n",
    "def extract_filename(filepath):\n",
    "    return filepath.split('/')[-1] \n",
    "\n",
    "def count_imports(imports):\n",
    "    imports_list = [imp for imp in imports.split(' | ') if imp]\n",
    "    return len(imports_list)\n",
    "\n",
    "def parse_imports(imports):\n",
    "    # Split the string by \" | \" and remove any empty strings\n",
    "    imports_list = [imp.strip().upper() for imp in imports.split(' | ') if imp.strip()]\n",
    "    for index, name in enumerate(imports_list):\n",
    "        if name.startswith(\"|\"):\n",
    "            name = name.replace(\"|\", \"\")\n",
    "            name = name.strip()\n",
    "            imports_list[index] = name.upper()\n",
    "        if re.search(r\"(.*)\\d-\\d-\\d.\\DLL\", name):\n",
    "            imports_list[index] = re.sub(r\"(.*)\\d-\\d-\\d.\\DLL\", r\"\\1X-X-X.DLL\", name)\n",
    "    return imports_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "malware_file_path = '../result/malware32.csv'\n",
    "df_malware = pd.read_csv(malware_file_path)\n",
    "df_malware['disposition'] = 'malware'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "bening_file_path = '../result/good32.csv'\n",
    "df_goodware = pd.read_csv(bening_file_path)\n",
    "df_goodware['disposition'] = 'goodware'\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "df = pd.concat([df_malware, df_goodware], ignore_index=True)\n",
    "# Verify the combined DataFrame\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(f\"Amount of Rows {df.shape[0]}\")\n",
    "print(\"First few rows of the dataset:\")\n",
    "df.head()\n",
    "\n",
    "# Handling missing values\n",
    "# Option 1: Drop missing values\n",
    "df = df.dropna()\n",
    "print(f\"Amount of Rows {df.shape[0]} after drop missing values\")\n",
    "\n",
    "# Checking for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Optionally remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Amount of Rows {df.shape[0]} After dropping duplicates\")\n",
    "\n",
    "# Convert column names to lowercase (optional)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Removing leading/trailing whitespaces from string columns\n",
    "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "#df = df['file_name'].apply(extract_filename)\n",
    "\n",
    "hex_columns = [\n",
    "    'table_pointer', 'size_of_uninitialized_data', 'address_of_entry_point',\n",
    "    'base_of_code', 'image_base'\n",
    "]\n",
    "\n",
    "for col in hex_columns:\n",
    "    df[col] = df[col].apply(hex_to_int)\n",
    "\n",
    "df['import_count'] = df['import_directory'].apply(count_imports)\n",
    "df['import_directory_list'] = df['import_directory'].apply(parse_imports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "row = df.iloc[1]\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Count occurrences of each item in the lists\n",
    "# all_imports = [item for sublist in df['import_directory_list'] for item in sublist]\n",
    "# item_counts = Counter(all_imports)\n",
    "\n",
    "# top_items = [item for item, count in item_counts.most_common(200)]\n",
    "# df['filtered_imports'] = df['import_directory_list'].apply(lambda x: [item for item in x if item in top_items])\n",
    "\n",
    "# # Step 4: One-Hot Encode the filtered lists\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# one_hot_encoded = mlb.fit_transform(df['filtered_imports'])\n",
    "\n",
    "# # Step 5: Create a DataFrame with meaningful column names\n",
    "# one_hot_df = pd.DataFrame(one_hot_encoded, columns=[f'import_{item}' for item in mlb.classes_])\n",
    "\n",
    "# # Step 6: Concatenate with the original DataFrame\n",
    "# final_df = pd.concat([df, one_hot_df], axis=1)\n",
    "# df = final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file_path = 'filtered_dataset.parquet'\n",
    "\n",
    "df.to_parquet(parquet_file_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"DataFrame successfully saved to {parquet_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
